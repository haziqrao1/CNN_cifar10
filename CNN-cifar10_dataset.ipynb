{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DL_A2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD1m9CVh9ShI"
      },
      "source": [
        "from matplotlib.pyplot import imread, imshow, imsave\n",
        "from keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPu8Wl0osAv7",
        "outputId": "006f84fa-3df3-4a90-9f6c-003efdc1d62b"
      },
      "source": [
        "(train_x,train_y),(test_x,test_y) = cifar10.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBKCIiDIsUvr",
        "outputId": "c575bba3-ef68-44eb-8bde-ab8b645bd583"
      },
      "source": [
        "print('Train X shape ',train_x.shape)\n",
        "print('Train Y shape ',train_y.shape)\n",
        "print('Test  X shape ',test_x.shape)\n",
        "print('Test  Y shape ',test_y.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train X shape  (50000, 32, 32, 3)\n",
            "Train Y shape  (50000, 1)\n",
            "Test  X shape  (10000, 32, 32, 3)\n",
            "Test  Y shape  (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "r-Vg2U6use6r",
        "outputId": "9dc8b873-a9dd-4e5d-848c-728f85993668"
      },
      "source": [
        "imshow(train_x[20])\n",
        "print(train_y[20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGkZIOUdtHi3"
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.optimizers import SGD\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9spjh4LP6EzW"
      },
      "source": [
        "Model CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6S0AqFgvkzh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1i27I92hR3S"
      },
      "source": [
        "num_classes = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz0liClmeTvI"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32, 32, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhOmXD9CwepH",
        "outputId": "75b32742-e2fb-41d1-9934-b714bf8d4b7c"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuT0UmVc_ZfP",
        "outputId": "a52d9861-8989-43e2-a506-dccccdd99a95"
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_x, train_y, epochs=10, \n",
        "                    validation_data=(test_x, test_y))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 44s 7ms/step - loss: 2.2801 - accuracy: 0.3893 - val_loss: 1.4174 - val_accuracy: 0.5683\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 1.2542 - accuracy: 0.6120 - val_loss: 0.9379 - val_accuracy: 0.7042\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.9504 - accuracy: 0.6984 - val_loss: 0.9232 - val_accuracy: 0.7156\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.8360 - accuracy: 0.7395 - val_loss: 0.8616 - val_accuracy: 0.7321\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7623 - accuracy: 0.7675 - val_loss: 0.8262 - val_accuracy: 0.7591\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.7121 - accuracy: 0.7903 - val_loss: 0.8704 - val_accuracy: 0.7482\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6899 - accuracy: 0.8066 - val_loss: 0.7336 - val_accuracy: 0.7950\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6671 - accuracy: 0.8209 - val_loss: 0.7170 - val_accuracy: 0.8120\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 10s 6ms/step - loss: 0.6417 - accuracy: 0.8312 - val_loss: 0.7449 - val_accuracy: 0.8081\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 10s 7ms/step - loss: 0.6343 - accuracy: 0.8384 - val_loss: 0.7209 - val_accuracy: 0.8161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5yPzqmjBNfw"
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.utils import np_utils\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import cifar10\n",
        "from keras import regularizers\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOVhdilt5mVH"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    lrate = 0.001\n",
        "    if epoch > 75:\n",
        "        lrate = 0.0005\n",
        "    if epoch > 100:\n",
        "        lrate = 0.0003\n",
        "    return lrate\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBLh5DOl9JZF"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5fyN0zEQ6Zl"
      },
      "source": [
        "#z-score\n",
        "mean = np.mean(x_train,axis=(0,1,2,3))\n",
        "std = np.std(x_train,axis=(0,1,2,3))\n",
        "x_train = (x_train-mean)/(std+1e-7)\n",
        "x_test = (x_test-mean)/(std+1e-7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITJAmn_CQqRH"
      },
      "source": [
        "\n",
        "num_classes = 10\n",
        "y_train = np_utils.to_categorical(y_train,num_classes)\n",
        "y_test = np_utils.to_categorical(y_test,num_classes)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-elhAef7Qyjc"
      },
      "source": [
        "weight_decay = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXH2lxKYQ_2d"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToW83CX2RGYX",
        "outputId": "7c78fb90-1dc0-43ca-e74e-7c743b4c41b1"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_24 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_25 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_26 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_27 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_28 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_29 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_14 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDPAAaLeRgZe"
      },
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKxWZ1U6Rogr"
      },
      "source": [
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um6tBlY0SCOb"
      },
      "source": [
        "#training\n",
        "batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJJRqhE1RsTi",
        "outputId": "7e5b7e27-a9fa-4620-fd2e-ca906043692e"
      },
      "source": [
        "opt_rms = keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=125,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "model.save_weights('model.h5') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/125\n",
            "781/781 [==============================] - 28s 35ms/step - loss: 2.3956 - accuracy: 0.3442 - val_loss: 1.3505 - val_accuracy: 0.5887\n",
            "Epoch 2/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 1.3836 - accuracy: 0.5692 - val_loss: 1.2127 - val_accuracy: 0.6459\n",
            "Epoch 3/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 1.1566 - accuracy: 0.6389 - val_loss: 1.0433 - val_accuracy: 0.6852\n",
            "Epoch 4/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9937 - accuracy: 0.6874 - val_loss: 0.9452 - val_accuracy: 0.7231\n",
            "Epoch 5/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.9333 - accuracy: 0.7148 - val_loss: 0.8405 - val_accuracy: 0.7528\n",
            "Epoch 6/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8848 - accuracy: 0.7289 - val_loss: 0.8002 - val_accuracy: 0.7651\n",
            "Epoch 7/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.8367 - accuracy: 0.7461 - val_loss: 0.7019 - val_accuracy: 0.7961\n",
            "Epoch 8/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.7991 - accuracy: 0.7616 - val_loss: 0.7376 - val_accuracy: 0.7889\n",
            "Epoch 9/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.7757 - accuracy: 0.7714 - val_loss: 0.7183 - val_accuracy: 0.8020\n",
            "Epoch 10/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7559 - accuracy: 0.7802 - val_loss: 0.7189 - val_accuracy: 0.7969\n",
            "Epoch 11/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.7369 - accuracy: 0.7845 - val_loss: 0.6631 - val_accuracy: 0.8206\n",
            "Epoch 12/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7321 - accuracy: 0.7864 - val_loss: 0.6885 - val_accuracy: 0.8122\n",
            "Epoch 13/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.7113 - accuracy: 0.7973 - val_loss: 0.6885 - val_accuracy: 0.8148\n",
            "Epoch 14/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.7058 - accuracy: 0.8025 - val_loss: 0.6831 - val_accuracy: 0.8182\n",
            "Epoch 15/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6811 - accuracy: 0.8071 - val_loss: 0.6903 - val_accuracy: 0.8165\n",
            "Epoch 16/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6843 - accuracy: 0.8108 - val_loss: 0.6546 - val_accuracy: 0.8259\n",
            "Epoch 17/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6825 - accuracy: 0.8099 - val_loss: 0.6235 - val_accuracy: 0.8372\n",
            "Epoch 18/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6663 - accuracy: 0.8127 - val_loss: 0.6887 - val_accuracy: 0.8203\n",
            "Epoch 19/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6607 - accuracy: 0.8171 - val_loss: 0.6654 - val_accuracy: 0.8241\n",
            "Epoch 20/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6543 - accuracy: 0.8220 - val_loss: 0.6626 - val_accuracy: 0.8218\n",
            "Epoch 21/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6457 - accuracy: 0.8246 - val_loss: 0.6323 - val_accuracy: 0.8334\n",
            "Epoch 22/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6455 - accuracy: 0.8252 - val_loss: 0.6577 - val_accuracy: 0.8320\n",
            "Epoch 23/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6428 - accuracy: 0.8259 - val_loss: 0.6603 - val_accuracy: 0.8281\n",
            "Epoch 24/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6462 - accuracy: 0.8271 - val_loss: 0.6307 - val_accuracy: 0.8380\n",
            "Epoch 25/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6336 - accuracy: 0.8326 - val_loss: 0.6341 - val_accuracy: 0.8348\n",
            "Epoch 26/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6334 - accuracy: 0.8306 - val_loss: 0.5696 - val_accuracy: 0.8568\n",
            "Epoch 27/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6244 - accuracy: 0.8344 - val_loss: 0.5728 - val_accuracy: 0.8564\n",
            "Epoch 28/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6201 - accuracy: 0.8383 - val_loss: 0.6118 - val_accuracy: 0.8453\n",
            "Epoch 29/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6231 - accuracy: 0.8346 - val_loss: 0.6549 - val_accuracy: 0.8327\n",
            "Epoch 30/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6219 - accuracy: 0.8342 - val_loss: 0.6224 - val_accuracy: 0.8434\n",
            "Epoch 31/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6241 - accuracy: 0.8361 - val_loss: 0.6020 - val_accuracy: 0.8516\n",
            "Epoch 32/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6214 - accuracy: 0.8389 - val_loss: 0.6091 - val_accuracy: 0.8509\n",
            "Epoch 33/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6197 - accuracy: 0.8369 - val_loss: 0.6248 - val_accuracy: 0.8411\n",
            "Epoch 34/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6060 - accuracy: 0.8421 - val_loss: 0.6056 - val_accuracy: 0.8488\n",
            "Epoch 35/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6137 - accuracy: 0.8403 - val_loss: 0.5937 - val_accuracy: 0.8536\n",
            "Epoch 36/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6140 - accuracy: 0.8406 - val_loss: 0.6296 - val_accuracy: 0.8435\n",
            "Epoch 37/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6084 - accuracy: 0.8441 - val_loss: 0.7465 - val_accuracy: 0.8119\n",
            "Epoch 38/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6018 - accuracy: 0.8442 - val_loss: 0.6427 - val_accuracy: 0.8405\n",
            "Epoch 39/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5987 - accuracy: 0.8458 - val_loss: 0.6134 - val_accuracy: 0.8509\n",
            "Epoch 40/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.6026 - accuracy: 0.8451 - val_loss: 0.6553 - val_accuracy: 0.8355\n",
            "Epoch 41/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6026 - accuracy: 0.8459 - val_loss: 0.6157 - val_accuracy: 0.8448\n",
            "Epoch 42/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6069 - accuracy: 0.8410 - val_loss: 0.7014 - val_accuracy: 0.8237\n",
            "Epoch 43/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6062 - accuracy: 0.8460 - val_loss: 0.6311 - val_accuracy: 0.8399\n",
            "Epoch 44/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.6080 - accuracy: 0.8440 - val_loss: 0.5770 - val_accuracy: 0.8582\n",
            "Epoch 45/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5974 - accuracy: 0.8460 - val_loss: 0.5938 - val_accuracy: 0.8542\n",
            "Epoch 46/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5905 - accuracy: 0.8498 - val_loss: 0.6290 - val_accuracy: 0.8470\n",
            "Epoch 47/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5983 - accuracy: 0.8490 - val_loss: 0.5799 - val_accuracy: 0.8596\n",
            "Epoch 48/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5910 - accuracy: 0.8486 - val_loss: 0.5876 - val_accuracy: 0.8589\n",
            "Epoch 49/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5942 - accuracy: 0.8467 - val_loss: 0.6322 - val_accuracy: 0.8489\n",
            "Epoch 50/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5862 - accuracy: 0.8532 - val_loss: 0.6442 - val_accuracy: 0.8424\n",
            "Epoch 51/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5886 - accuracy: 0.8494 - val_loss: 0.6194 - val_accuracy: 0.8429\n",
            "Epoch 52/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5903 - accuracy: 0.8511 - val_loss: 0.6193 - val_accuracy: 0.8525\n",
            "Epoch 53/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5842 - accuracy: 0.8536 - val_loss: 0.6159 - val_accuracy: 0.8487\n",
            "Epoch 54/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5845 - accuracy: 0.8504 - val_loss: 0.5993 - val_accuracy: 0.8550\n",
            "Epoch 55/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5874 - accuracy: 0.8513 - val_loss: 0.6159 - val_accuracy: 0.8491\n",
            "Epoch 56/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5873 - accuracy: 0.8509 - val_loss: 0.5694 - val_accuracy: 0.8638\n",
            "Epoch 57/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5859 - accuracy: 0.8492 - val_loss: 0.6142 - val_accuracy: 0.8516\n",
            "Epoch 58/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5944 - accuracy: 0.8499 - val_loss: 0.5932 - val_accuracy: 0.8576\n",
            "Epoch 59/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5819 - accuracy: 0.8534 - val_loss: 0.6118 - val_accuracy: 0.8458\n",
            "Epoch 60/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5870 - accuracy: 0.8518 - val_loss: 0.6104 - val_accuracy: 0.8539\n",
            "Epoch 61/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5825 - accuracy: 0.8535 - val_loss: 0.5827 - val_accuracy: 0.8584\n",
            "Epoch 62/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5801 - accuracy: 0.8545 - val_loss: 0.5630 - val_accuracy: 0.8647\n",
            "Epoch 63/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5828 - accuracy: 0.8547 - val_loss: 0.6302 - val_accuracy: 0.8445\n",
            "Epoch 64/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5849 - accuracy: 0.8523 - val_loss: 0.6341 - val_accuracy: 0.8500\n",
            "Epoch 65/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5787 - accuracy: 0.8563 - val_loss: 0.6778 - val_accuracy: 0.8333\n",
            "Epoch 66/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5721 - accuracy: 0.8561 - val_loss: 0.6005 - val_accuracy: 0.8598\n",
            "Epoch 67/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5801 - accuracy: 0.8557 - val_loss: 0.6289 - val_accuracy: 0.8484\n",
            "Epoch 68/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5837 - accuracy: 0.8540 - val_loss: 0.5762 - val_accuracy: 0.8634\n",
            "Epoch 69/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5778 - accuracy: 0.8550 - val_loss: 0.6931 - val_accuracy: 0.8280\n",
            "Epoch 70/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5773 - accuracy: 0.8563 - val_loss: 0.6167 - val_accuracy: 0.8504\n",
            "Epoch 71/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5734 - accuracy: 0.8584 - val_loss: 0.6017 - val_accuracy: 0.8573\n",
            "Epoch 72/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5820 - accuracy: 0.8537 - val_loss: 0.6147 - val_accuracy: 0.8479\n",
            "Epoch 73/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5851 - accuracy: 0.8523 - val_loss: 0.6344 - val_accuracy: 0.8471\n",
            "Epoch 74/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5665 - accuracy: 0.8591 - val_loss: 0.6030 - val_accuracy: 0.8563\n",
            "Epoch 75/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5665 - accuracy: 0.8613 - val_loss: 0.6519 - val_accuracy: 0.8445\n",
            "Epoch 76/125\n",
            "781/781 [==============================] - 26s 34ms/step - loss: 0.5758 - accuracy: 0.8553 - val_loss: 0.6026 - val_accuracy: 0.8550\n",
            "Epoch 77/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5338 - accuracy: 0.8688 - val_loss: 0.5856 - val_accuracy: 0.8587\n",
            "Epoch 78/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5069 - accuracy: 0.8782 - val_loss: 0.5392 - val_accuracy: 0.8724\n",
            "Epoch 79/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5027 - accuracy: 0.8781 - val_loss: 0.5161 - val_accuracy: 0.8797\n",
            "Epoch 80/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.5023 - accuracy: 0.8782 - val_loss: 0.5180 - val_accuracy: 0.8797\n",
            "Epoch 81/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4844 - accuracy: 0.8819 - val_loss: 0.5088 - val_accuracy: 0.8801\n",
            "Epoch 82/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4849 - accuracy: 0.8808 - val_loss: 0.5715 - val_accuracy: 0.8622\n",
            "Epoch 83/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4905 - accuracy: 0.8761 - val_loss: 0.5419 - val_accuracy: 0.8693\n",
            "Epoch 84/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4778 - accuracy: 0.8821 - val_loss: 0.5423 - val_accuracy: 0.8703\n",
            "Epoch 85/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4721 - accuracy: 0.8850 - val_loss: 0.5327 - val_accuracy: 0.8705\n",
            "Epoch 86/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4740 - accuracy: 0.8809 - val_loss: 0.5100 - val_accuracy: 0.8767\n",
            "Epoch 87/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4748 - accuracy: 0.8810 - val_loss: 0.4868 - val_accuracy: 0.8839\n",
            "Epoch 88/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4674 - accuracy: 0.8833 - val_loss: 0.5276 - val_accuracy: 0.8717\n",
            "Epoch 89/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4650 - accuracy: 0.8835 - val_loss: 0.5632 - val_accuracy: 0.8640\n",
            "Epoch 90/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4614 - accuracy: 0.8848 - val_loss: 0.5194 - val_accuracy: 0.8736\n",
            "Epoch 91/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4620 - accuracy: 0.8841 - val_loss: 0.5305 - val_accuracy: 0.8717\n",
            "Epoch 92/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4648 - accuracy: 0.8831 - val_loss: 0.4874 - val_accuracy: 0.8829\n",
            "Epoch 93/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4527 - accuracy: 0.8843 - val_loss: 0.4928 - val_accuracy: 0.8794\n",
            "Epoch 94/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4605 - accuracy: 0.8818 - val_loss: 0.4802 - val_accuracy: 0.8875\n",
            "Epoch 95/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4568 - accuracy: 0.8850 - val_loss: 0.5209 - val_accuracy: 0.8755\n",
            "Epoch 96/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4611 - accuracy: 0.8839 - val_loss: 0.5152 - val_accuracy: 0.8777\n",
            "Epoch 97/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4595 - accuracy: 0.8828 - val_loss: 0.5098 - val_accuracy: 0.8785\n",
            "Epoch 98/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4494 - accuracy: 0.8865 - val_loss: 0.4959 - val_accuracy: 0.8808\n",
            "Epoch 99/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4526 - accuracy: 0.8868 - val_loss: 0.5062 - val_accuracy: 0.8778\n",
            "Epoch 100/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4538 - accuracy: 0.8851 - val_loss: 0.4947 - val_accuracy: 0.8808\n",
            "Epoch 101/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4452 - accuracy: 0.8885 - val_loss: 0.4773 - val_accuracy: 0.8838\n",
            "Epoch 102/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4344 - accuracy: 0.8904 - val_loss: 0.4748 - val_accuracy: 0.8853\n",
            "Epoch 103/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4170 - accuracy: 0.8958 - val_loss: 0.4659 - val_accuracy: 0.8877\n",
            "Epoch 104/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4149 - accuracy: 0.8956 - val_loss: 0.4714 - val_accuracy: 0.8856\n",
            "Epoch 105/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4134 - accuracy: 0.8964 - val_loss: 0.4981 - val_accuracy: 0.8783\n",
            "Epoch 106/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4092 - accuracy: 0.8977 - val_loss: 0.4926 - val_accuracy: 0.8826\n",
            "Epoch 107/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4101 - accuracy: 0.8952 - val_loss: 0.4695 - val_accuracy: 0.8897\n",
            "Epoch 108/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4102 - accuracy: 0.8953 - val_loss: 0.4884 - val_accuracy: 0.8814\n",
            "Epoch 109/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4072 - accuracy: 0.8972 - val_loss: 0.4528 - val_accuracy: 0.8905\n",
            "Epoch 110/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4012 - accuracy: 0.8984 - val_loss: 0.4996 - val_accuracy: 0.8790\n",
            "Epoch 111/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4023 - accuracy: 0.8977 - val_loss: 0.4686 - val_accuracy: 0.8849\n",
            "Epoch 112/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3949 - accuracy: 0.9005 - val_loss: 0.4451 - val_accuracy: 0.8953\n",
            "Epoch 113/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4057 - accuracy: 0.8973 - val_loss: 0.4613 - val_accuracy: 0.8888\n",
            "Epoch 114/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.4011 - accuracy: 0.8980 - val_loss: 0.4614 - val_accuracy: 0.8886\n",
            "Epoch 115/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3908 - accuracy: 0.9012 - val_loss: 0.4764 - val_accuracy: 0.8825\n",
            "Epoch 116/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3933 - accuracy: 0.8996 - val_loss: 0.5089 - val_accuracy: 0.8767\n",
            "Epoch 117/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.4001 - accuracy: 0.8982 - val_loss: 0.4878 - val_accuracy: 0.8793\n",
            "Epoch 118/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3935 - accuracy: 0.9016 - val_loss: 0.4715 - val_accuracy: 0.8840\n",
            "Epoch 119/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3892 - accuracy: 0.9013 - val_loss: 0.4668 - val_accuracy: 0.8850\n",
            "Epoch 120/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3854 - accuracy: 0.9008 - val_loss: 0.4581 - val_accuracy: 0.8888\n",
            "Epoch 121/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3888 - accuracy: 0.9018 - val_loss: 0.4950 - val_accuracy: 0.8783\n",
            "Epoch 122/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3881 - accuracy: 0.9002 - val_loss: 0.4555 - val_accuracy: 0.8870\n",
            "Epoch 123/125\n",
            "781/781 [==============================] - 27s 35ms/step - loss: 0.3862 - accuracy: 0.9014 - val_loss: 0.4537 - val_accuracy: 0.8878\n",
            "Epoch 124/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3871 - accuracy: 0.9000 - val_loss: 0.4508 - val_accuracy: 0.8870\n",
            "Epoch 125/125\n",
            "781/781 [==============================] - 27s 34ms/step - loss: 0.3946 - accuracy: 0.8976 - val_loss: 0.4773 - val_accuracy: 0.8804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEycbHS0R0qR",
        "outputId": "5b598a24-cbf5-463c-ace3-afb8e5b938eb"
      },
      "source": [
        " #testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 1s 5ms/step - loss: 0.4773 - accuracy: 0.8804\n",
            "\n",
            "Test result: 88.040 loss: 0.477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "QN_Rw-T-V2sy",
        "outputId": "87b907c9-3099-4db2-de48-18d99124ae98"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "model.load_weights('model.h5')\n",
        " \n",
        "labels =  ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        " \n",
        "indices = np.argmax(model.predict(x_test[:26]),1)\n",
        "print([labels[x] for x in indices])\n",
        "imshow(x_test[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['cat', 'ship', 'ship', 'airplane', 'frog', 'frog', 'automobile', 'frog', 'cat', 'automobile', 'airplane', 'truck', 'dog', 'horse', 'truck', 'ship', 'dog', 'horse', 'ship', 'frog', 'horse', 'airplane', 'deer', 'truck', 'deer', 'bird']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f88173bf150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NnHyIy5oX8V"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(32, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZIEm4JApk5R",
        "outputId": "20bcaa1b-830d-4757-bba4-1302be947dc2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_78 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_63 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_79 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_64 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_36 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_80 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_65 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_81 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_57 (Activation)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_37 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_82 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_58 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_83 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_59 (Activation)   (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_38 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1024)              2098176   \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 512)               524800    \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 2,923,050\n",
            "Trainable params: 2,919,082\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlzPTgGfrMkQ"
      },
      "source": [
        "#data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUA3WYsRrVbz"
      },
      "source": [
        "datagen.fit(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75RzzALerZ-W"
      },
      "source": [
        "batch_size=64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWFSb_CFrcKm",
        "outputId": "68f1e9f6-4253-4a5d-fe65-82039c75ba7e"
      },
      "source": [
        "opt_rms = keras.optimizers.RMSprop(lr=0.001,decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt_rms, metrics=['accuracy'])\n",
        "model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\\\n",
        "                    steps_per_epoch=x_train.shape[0] // batch_size,epochs=300,\\\n",
        "                    verbose=1,validation_data=(x_test,y_test),callbacks=[LearningRateScheduler(lr_schedule)])\n",
        "model.save_weights('model.h5') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "781/781 [==============================] - 30s 36ms/step - loss: 0.9819 - accuracy: 0.6889 - val_loss: 0.9619 - val_accuracy: 0.7099\n",
            "Epoch 2/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.9290 - accuracy: 0.7140 - val_loss: 0.7714 - val_accuracy: 0.7732\n",
            "Epoch 3/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.9044 - accuracy: 0.7248 - val_loss: 0.8493 - val_accuracy: 0.7583\n",
            "Epoch 4/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.8885 - accuracy: 0.7319 - val_loss: 0.7948 - val_accuracy: 0.7723\n",
            "Epoch 5/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.8587 - accuracy: 0.7459 - val_loss: 0.8509 - val_accuracy: 0.7656\n",
            "Epoch 6/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.8399 - accuracy: 0.7545 - val_loss: 0.8063 - val_accuracy: 0.7764\n",
            "Epoch 7/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.8225 - accuracy: 0.7613 - val_loss: 0.7786 - val_accuracy: 0.7822\n",
            "Epoch 8/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.8184 - accuracy: 0.7613 - val_loss: 0.7961 - val_accuracy: 0.7842\n",
            "Epoch 9/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.8073 - accuracy: 0.7660 - val_loss: 0.7087 - val_accuracy: 0.8107\n",
            "Epoch 10/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.8037 - accuracy: 0.7705 - val_loss: 0.7040 - val_accuracy: 0.8149\n",
            "Epoch 11/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7967 - accuracy: 0.7733 - val_loss: 0.8338 - val_accuracy: 0.7733\n",
            "Epoch 12/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.7800 - accuracy: 0.7804 - val_loss: 0.7961 - val_accuracy: 0.7890\n",
            "Epoch 13/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.7794 - accuracy: 0.7819 - val_loss: 0.7308 - val_accuracy: 0.8081\n",
            "Epoch 14/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7686 - accuracy: 0.7880 - val_loss: 0.6445 - val_accuracy: 0.8336\n",
            "Epoch 15/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.7568 - accuracy: 0.7908 - val_loss: 0.6779 - val_accuracy: 0.8237\n",
            "Epoch 16/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7592 - accuracy: 0.7915 - val_loss: 0.7120 - val_accuracy: 0.8135\n",
            "Epoch 17/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.7548 - accuracy: 0.7922 - val_loss: 0.6392 - val_accuracy: 0.8356\n",
            "Epoch 18/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7507 - accuracy: 0.7966 - val_loss: 0.7102 - val_accuracy: 0.8167\n",
            "Epoch 19/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7466 - accuracy: 0.7973 - val_loss: 0.6559 - val_accuracy: 0.8307\n",
            "Epoch 20/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7384 - accuracy: 0.7980 - val_loss: 0.6748 - val_accuracy: 0.8265\n",
            "Epoch 21/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7348 - accuracy: 0.8033 - val_loss: 0.6530 - val_accuracy: 0.8344\n",
            "Epoch 22/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7302 - accuracy: 0.8036 - val_loss: 0.6558 - val_accuracy: 0.8361\n",
            "Epoch 23/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7275 - accuracy: 0.8043 - val_loss: 0.6425 - val_accuracy: 0.8386\n",
            "Epoch 24/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7261 - accuracy: 0.8042 - val_loss: 0.7176 - val_accuracy: 0.8194\n",
            "Epoch 25/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7213 - accuracy: 0.8049 - val_loss: 0.7331 - val_accuracy: 0.8177\n",
            "Epoch 26/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7255 - accuracy: 0.8060 - val_loss: 0.6278 - val_accuracy: 0.8431\n",
            "Epoch 27/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7210 - accuracy: 0.8086 - val_loss: 0.6234 - val_accuracy: 0.8469\n",
            "Epoch 28/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7215 - accuracy: 0.8072 - val_loss: 0.6874 - val_accuracy: 0.8245\n",
            "Epoch 29/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7090 - accuracy: 0.8127 - val_loss: 0.6650 - val_accuracy: 0.8332\n",
            "Epoch 30/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7069 - accuracy: 0.8141 - val_loss: 0.7038 - val_accuracy: 0.8226\n",
            "Epoch 31/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7142 - accuracy: 0.8106 - val_loss: 0.6086 - val_accuracy: 0.8492\n",
            "Epoch 32/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7073 - accuracy: 0.8144 - val_loss: 0.6912 - val_accuracy: 0.8264\n",
            "Epoch 33/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7069 - accuracy: 0.8163 - val_loss: 0.6956 - val_accuracy: 0.8256\n",
            "Epoch 34/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7008 - accuracy: 0.8144 - val_loss: 0.6131 - val_accuracy: 0.8499\n",
            "Epoch 35/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6929 - accuracy: 0.8187 - val_loss: 0.7864 - val_accuracy: 0.8069\n",
            "Epoch 36/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.7098 - accuracy: 0.8131 - val_loss: 0.6449 - val_accuracy: 0.8414\n",
            "Epoch 37/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.7034 - accuracy: 0.8138 - val_loss: 0.6840 - val_accuracy: 0.8321\n",
            "Epoch 38/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6958 - accuracy: 0.8169 - val_loss: 0.6341 - val_accuracy: 0.8452\n",
            "Epoch 39/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6945 - accuracy: 0.8185 - val_loss: 0.6031 - val_accuracy: 0.8517\n",
            "Epoch 40/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6897 - accuracy: 0.8191 - val_loss: 0.6344 - val_accuracy: 0.8418\n",
            "Epoch 41/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6894 - accuracy: 0.8183 - val_loss: 0.6853 - val_accuracy: 0.8407\n",
            "Epoch 42/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6970 - accuracy: 0.8197 - val_loss: 0.6316 - val_accuracy: 0.8463\n",
            "Epoch 43/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6843 - accuracy: 0.8207 - val_loss: 0.6491 - val_accuracy: 0.8443\n",
            "Epoch 44/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6884 - accuracy: 0.8224 - val_loss: 0.6237 - val_accuracy: 0.8520\n",
            "Epoch 45/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6784 - accuracy: 0.8246 - val_loss: 0.6162 - val_accuracy: 0.8505\n",
            "Epoch 46/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6798 - accuracy: 0.8224 - val_loss: 0.6555 - val_accuracy: 0.8376\n",
            "Epoch 47/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6865 - accuracy: 0.8219 - val_loss: 0.6388 - val_accuracy: 0.8443\n",
            "Epoch 48/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6778 - accuracy: 0.8246 - val_loss: 0.7683 - val_accuracy: 0.8125\n",
            "Epoch 49/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6685 - accuracy: 0.8288 - val_loss: 0.5996 - val_accuracy: 0.8594\n",
            "Epoch 50/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6706 - accuracy: 0.8247 - val_loss: 0.6134 - val_accuracy: 0.8512\n",
            "Epoch 51/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6705 - accuracy: 0.8254 - val_loss: 0.6036 - val_accuracy: 0.8514\n",
            "Epoch 52/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6790 - accuracy: 0.8239 - val_loss: 0.6519 - val_accuracy: 0.8435\n",
            "Epoch 53/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6765 - accuracy: 0.8240 - val_loss: 0.6443 - val_accuracy: 0.8410\n",
            "Epoch 54/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6693 - accuracy: 0.8260 - val_loss: 0.6440 - val_accuracy: 0.8439\n",
            "Epoch 55/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6730 - accuracy: 0.8246 - val_loss: 0.6034 - val_accuracy: 0.8524\n",
            "Epoch 56/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6691 - accuracy: 0.8274 - val_loss: 0.6104 - val_accuracy: 0.8528\n",
            "Epoch 57/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6709 - accuracy: 0.8258 - val_loss: 0.7229 - val_accuracy: 0.8248\n",
            "Epoch 58/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6694 - accuracy: 0.8280 - val_loss: 0.7281 - val_accuracy: 0.8214\n",
            "Epoch 59/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6729 - accuracy: 0.8255 - val_loss: 0.5877 - val_accuracy: 0.8603\n",
            "Epoch 60/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6730 - accuracy: 0.8253 - val_loss: 0.6093 - val_accuracy: 0.8559\n",
            "Epoch 61/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6638 - accuracy: 0.8286 - val_loss: 0.6194 - val_accuracy: 0.8518\n",
            "Epoch 62/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6590 - accuracy: 0.8292 - val_loss: 0.6632 - val_accuracy: 0.8411\n",
            "Epoch 63/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6658 - accuracy: 0.8297 - val_loss: 0.6303 - val_accuracy: 0.8524\n",
            "Epoch 64/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6565 - accuracy: 0.8302 - val_loss: 0.6526 - val_accuracy: 0.8388\n",
            "Epoch 65/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6690 - accuracy: 0.8256 - val_loss: 0.6039 - val_accuracy: 0.8572\n",
            "Epoch 66/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6528 - accuracy: 0.8307 - val_loss: 0.6383 - val_accuracy: 0.8496\n",
            "Epoch 67/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6655 - accuracy: 0.8311 - val_loss: 0.6199 - val_accuracy: 0.8498\n",
            "Epoch 68/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6565 - accuracy: 0.8298 - val_loss: 0.5900 - val_accuracy: 0.8597\n",
            "Epoch 69/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6510 - accuracy: 0.8329 - val_loss: 0.6179 - val_accuracy: 0.8541\n",
            "Epoch 70/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6585 - accuracy: 0.8286 - val_loss: 0.5911 - val_accuracy: 0.8550\n",
            "Epoch 71/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6538 - accuracy: 0.8329 - val_loss: 0.6146 - val_accuracy: 0.8542\n",
            "Epoch 72/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6504 - accuracy: 0.8336 - val_loss: 0.5537 - val_accuracy: 0.8696\n",
            "Epoch 73/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.6431 - accuracy: 0.8348 - val_loss: 0.6580 - val_accuracy: 0.8401\n",
            "Epoch 74/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6440 - accuracy: 0.8381 - val_loss: 0.5992 - val_accuracy: 0.8515\n",
            "Epoch 75/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6541 - accuracy: 0.8327 - val_loss: 0.6032 - val_accuracy: 0.8535\n",
            "Epoch 76/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.6556 - accuracy: 0.8330 - val_loss: 0.5923 - val_accuracy: 0.8590\n",
            "Epoch 77/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.6143 - accuracy: 0.8467 - val_loss: 0.5569 - val_accuracy: 0.8678\n",
            "Epoch 78/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5996 - accuracy: 0.8477 - val_loss: 0.5348 - val_accuracy: 0.8719\n",
            "Epoch 79/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5849 - accuracy: 0.8542 - val_loss: 0.5904 - val_accuracy: 0.8623\n",
            "Epoch 80/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5769 - accuracy: 0.8529 - val_loss: 0.5084 - val_accuracy: 0.8828\n",
            "Epoch 81/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5761 - accuracy: 0.8546 - val_loss: 0.5281 - val_accuracy: 0.8749\n",
            "Epoch 82/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.5589 - accuracy: 0.8607 - val_loss: 0.5610 - val_accuracy: 0.8661\n",
            "Epoch 83/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.5693 - accuracy: 0.8547 - val_loss: 0.5278 - val_accuracy: 0.8714\n",
            "Epoch 84/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.5617 - accuracy: 0.8579 - val_loss: 0.5462 - val_accuracy: 0.8669\n",
            "Epoch 85/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.5569 - accuracy: 0.8573 - val_loss: 0.5301 - val_accuracy: 0.8710\n",
            "Epoch 86/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.5506 - accuracy: 0.8595 - val_loss: 0.5348 - val_accuracy: 0.8695\n",
            "Epoch 87/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5477 - accuracy: 0.8593 - val_loss: 0.4797 - val_accuracy: 0.8845\n",
            "Epoch 88/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5522 - accuracy: 0.8573 - val_loss: 0.5181 - val_accuracy: 0.8739\n",
            "Epoch 89/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5492 - accuracy: 0.8603 - val_loss: 0.5073 - val_accuracy: 0.8779\n",
            "Epoch 90/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5435 - accuracy: 0.8602 - val_loss: 0.5325 - val_accuracy: 0.8717\n",
            "Epoch 91/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5439 - accuracy: 0.8599 - val_loss: 0.4938 - val_accuracy: 0.8800\n",
            "Epoch 92/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5413 - accuracy: 0.8578 - val_loss: 0.5373 - val_accuracy: 0.8708\n",
            "Epoch 93/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.5348 - accuracy: 0.8611 - val_loss: 0.5601 - val_accuracy: 0.8619\n",
            "Epoch 94/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5423 - accuracy: 0.8594 - val_loss: 0.5253 - val_accuracy: 0.8713\n",
            "Epoch 95/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5332 - accuracy: 0.8617 - val_loss: 0.5203 - val_accuracy: 0.8734\n",
            "Epoch 96/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5289 - accuracy: 0.8645 - val_loss: 0.5498 - val_accuracy: 0.8667\n",
            "Epoch 97/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5340 - accuracy: 0.8634 - val_loss: 0.5627 - val_accuracy: 0.8632\n",
            "Epoch 98/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5249 - accuracy: 0.8602 - val_loss: 0.5476 - val_accuracy: 0.8672\n",
            "Epoch 99/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5295 - accuracy: 0.8602 - val_loss: 0.5145 - val_accuracy: 0.8736\n",
            "Epoch 100/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5369 - accuracy: 0.8587 - val_loss: 0.5373 - val_accuracy: 0.8694\n",
            "Epoch 101/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5253 - accuracy: 0.8626 - val_loss: 0.4890 - val_accuracy: 0.8803\n",
            "Epoch 102/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5179 - accuracy: 0.8656 - val_loss: 0.4873 - val_accuracy: 0.8812\n",
            "Epoch 103/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4936 - accuracy: 0.8700 - val_loss: 0.4790 - val_accuracy: 0.8837\n",
            "Epoch 104/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4995 - accuracy: 0.8690 - val_loss: 0.4810 - val_accuracy: 0.8791\n",
            "Epoch 105/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4925 - accuracy: 0.8724 - val_loss: 0.4808 - val_accuracy: 0.8796\n",
            "Epoch 106/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.5000 - accuracy: 0.8701 - val_loss: 0.4591 - val_accuracy: 0.8871\n",
            "Epoch 107/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4921 - accuracy: 0.8723 - val_loss: 0.5160 - val_accuracy: 0.8747\n",
            "Epoch 108/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4881 - accuracy: 0.8727 - val_loss: 0.4695 - val_accuracy: 0.8836\n",
            "Epoch 109/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4851 - accuracy: 0.8745 - val_loss: 0.4712 - val_accuracy: 0.8843\n",
            "Epoch 110/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4848 - accuracy: 0.8726 - val_loss: 0.4782 - val_accuracy: 0.8825\n",
            "Epoch 111/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4844 - accuracy: 0.8746 - val_loss: 0.4815 - val_accuracy: 0.8815\n",
            "Epoch 112/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4849 - accuracy: 0.8723 - val_loss: 0.4756 - val_accuracy: 0.8821\n",
            "Epoch 113/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4823 - accuracy: 0.8745 - val_loss: 0.4530 - val_accuracy: 0.8850\n",
            "Epoch 114/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4784 - accuracy: 0.8750 - val_loss: 0.4614 - val_accuracy: 0.8848\n",
            "Epoch 115/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4849 - accuracy: 0.8728 - val_loss: 0.4718 - val_accuracy: 0.8842\n",
            "Epoch 116/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4761 - accuracy: 0.8768 - val_loss: 0.4588 - val_accuracy: 0.8841\n",
            "Epoch 117/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4854 - accuracy: 0.8712 - val_loss: 0.4726 - val_accuracy: 0.8828\n",
            "Epoch 118/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4714 - accuracy: 0.8749 - val_loss: 0.4664 - val_accuracy: 0.8850\n",
            "Epoch 119/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4735 - accuracy: 0.8759 - val_loss: 0.4867 - val_accuracy: 0.8782\n",
            "Epoch 120/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4749 - accuracy: 0.8770 - val_loss: 0.4516 - val_accuracy: 0.8894\n",
            "Epoch 121/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4733 - accuracy: 0.8734 - val_loss: 0.4352 - val_accuracy: 0.8931\n",
            "Epoch 122/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4683 - accuracy: 0.8773 - val_loss: 0.4655 - val_accuracy: 0.8823\n",
            "Epoch 123/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4694 - accuracy: 0.8753 - val_loss: 0.4241 - val_accuracy: 0.8952\n",
            "Epoch 124/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4670 - accuracy: 0.8756 - val_loss: 0.4709 - val_accuracy: 0.8819\n",
            "Epoch 125/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4753 - accuracy: 0.8753 - val_loss: 0.4504 - val_accuracy: 0.8867\n",
            "Epoch 126/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4624 - accuracy: 0.8790 - val_loss: 0.4488 - val_accuracy: 0.8895\n",
            "Epoch 127/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4640 - accuracy: 0.8784 - val_loss: 0.4427 - val_accuracy: 0.8885\n",
            "Epoch 128/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4684 - accuracy: 0.8754 - val_loss: 0.4630 - val_accuracy: 0.8857\n",
            "Epoch 129/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4599 - accuracy: 0.8782 - val_loss: 0.4450 - val_accuracy: 0.8857\n",
            "Epoch 130/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4629 - accuracy: 0.8784 - val_loss: 0.4497 - val_accuracy: 0.8885\n",
            "Epoch 131/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4549 - accuracy: 0.8796 - val_loss: 0.4240 - val_accuracy: 0.8957\n",
            "Epoch 132/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4681 - accuracy: 0.8755 - val_loss: 0.4391 - val_accuracy: 0.8907\n",
            "Epoch 133/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4516 - accuracy: 0.8784 - val_loss: 0.4344 - val_accuracy: 0.8913\n",
            "Epoch 134/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4589 - accuracy: 0.8779 - val_loss: 0.4628 - val_accuracy: 0.8855\n",
            "Epoch 135/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4580 - accuracy: 0.8790 - val_loss: 0.4770 - val_accuracy: 0.8804\n",
            "Epoch 136/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4534 - accuracy: 0.8813 - val_loss: 0.4357 - val_accuracy: 0.8905\n",
            "Epoch 137/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4590 - accuracy: 0.8793 - val_loss: 0.4660 - val_accuracy: 0.8824\n",
            "Epoch 138/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4491 - accuracy: 0.8804 - val_loss: 0.4424 - val_accuracy: 0.8872\n",
            "Epoch 139/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4541 - accuracy: 0.8779 - val_loss: 0.4416 - val_accuracy: 0.8900\n",
            "Epoch 140/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4538 - accuracy: 0.8799 - val_loss: 0.4591 - val_accuracy: 0.8839\n",
            "Epoch 141/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4498 - accuracy: 0.8796 - val_loss: 0.4585 - val_accuracy: 0.8856\n",
            "Epoch 142/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4476 - accuracy: 0.8817 - val_loss: 0.4585 - val_accuracy: 0.8877\n",
            "Epoch 143/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4528 - accuracy: 0.8809 - val_loss: 0.4715 - val_accuracy: 0.8852\n",
            "Epoch 144/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4555 - accuracy: 0.8782 - val_loss: 0.4435 - val_accuracy: 0.8890\n",
            "Epoch 145/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4542 - accuracy: 0.8781 - val_loss: 0.4676 - val_accuracy: 0.8828\n",
            "Epoch 146/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4634 - accuracy: 0.8783 - val_loss: 0.4824 - val_accuracy: 0.8782\n",
            "Epoch 147/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4501 - accuracy: 0.8797 - val_loss: 0.4743 - val_accuracy: 0.8836\n",
            "Epoch 148/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4596 - accuracy: 0.8779 - val_loss: 0.4494 - val_accuracy: 0.8906\n",
            "Epoch 149/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4495 - accuracy: 0.8791 - val_loss: 0.4227 - val_accuracy: 0.8952\n",
            "Epoch 150/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4535 - accuracy: 0.8771 - val_loss: 0.4643 - val_accuracy: 0.8813\n",
            "Epoch 151/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4380 - accuracy: 0.8835 - val_loss: 0.4753 - val_accuracy: 0.8783\n",
            "Epoch 152/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4507 - accuracy: 0.8807 - val_loss: 0.4425 - val_accuracy: 0.8888\n",
            "Epoch 153/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4480 - accuracy: 0.8799 - val_loss: 0.4463 - val_accuracy: 0.8890\n",
            "Epoch 154/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4421 - accuracy: 0.8800 - val_loss: 0.4611 - val_accuracy: 0.8840\n",
            "Epoch 155/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4356 - accuracy: 0.8837 - val_loss: 0.4595 - val_accuracy: 0.8844\n",
            "Epoch 156/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4374 - accuracy: 0.8803 - val_loss: 0.4414 - val_accuracy: 0.8890\n",
            "Epoch 157/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4384 - accuracy: 0.8799 - val_loss: 0.4773 - val_accuracy: 0.8821\n",
            "Epoch 158/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4486 - accuracy: 0.8800 - val_loss: 0.4113 - val_accuracy: 0.8946\n",
            "Epoch 159/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4534 - accuracy: 0.8752 - val_loss: 0.4538 - val_accuracy: 0.8847\n",
            "Epoch 160/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4460 - accuracy: 0.8800 - val_loss: 0.4395 - val_accuracy: 0.8905\n",
            "Epoch 161/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4382 - accuracy: 0.8835 - val_loss: 0.4534 - val_accuracy: 0.8841\n",
            "Epoch 162/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4505 - accuracy: 0.8783 - val_loss: 0.4232 - val_accuracy: 0.8942\n",
            "Epoch 163/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4388 - accuracy: 0.8815 - val_loss: 0.4327 - val_accuracy: 0.8924\n",
            "Epoch 164/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4460 - accuracy: 0.8820 - val_loss: 0.4316 - val_accuracy: 0.8931\n",
            "Epoch 165/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4435 - accuracy: 0.8802 - val_loss: 0.4394 - val_accuracy: 0.8899\n",
            "Epoch 166/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4375 - accuracy: 0.8823 - val_loss: 0.4714 - val_accuracy: 0.8816\n",
            "Epoch 167/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4381 - accuracy: 0.8823 - val_loss: 0.4478 - val_accuracy: 0.8857\n",
            "Epoch 168/300\n",
            "781/781 [==============================] - 29s 36ms/step - loss: 0.4387 - accuracy: 0.8833 - val_loss: 0.4384 - val_accuracy: 0.8891\n",
            "Epoch 169/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4355 - accuracy: 0.8809 - val_loss: 0.4160 - val_accuracy: 0.8967\n",
            "Epoch 170/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4433 - accuracy: 0.8805 - val_loss: 0.4361 - val_accuracy: 0.8903\n",
            "Epoch 171/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4469 - accuracy: 0.8790 - val_loss: 0.4498 - val_accuracy: 0.8856\n",
            "Epoch 172/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4450 - accuracy: 0.8815 - val_loss: 0.4277 - val_accuracy: 0.8904\n",
            "Epoch 173/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4419 - accuracy: 0.8800 - val_loss: 0.4598 - val_accuracy: 0.8844\n",
            "Epoch 174/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4329 - accuracy: 0.8812 - val_loss: 0.4583 - val_accuracy: 0.8836\n",
            "Epoch 175/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4370 - accuracy: 0.8844 - val_loss: 0.4421 - val_accuracy: 0.8892\n",
            "Epoch 176/300\n",
            "781/781 [==============================] - 29s 36ms/step - loss: 0.4390 - accuracy: 0.8796 - val_loss: 0.4518 - val_accuracy: 0.8870\n",
            "Epoch 177/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4323 - accuracy: 0.8843 - val_loss: 0.4840 - val_accuracy: 0.8784\n",
            "Epoch 178/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4433 - accuracy: 0.8811 - val_loss: 0.4542 - val_accuracy: 0.8854\n",
            "Epoch 179/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4295 - accuracy: 0.8861 - val_loss: 0.4553 - val_accuracy: 0.8836\n",
            "Epoch 180/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4409 - accuracy: 0.8801 - val_loss: 0.4370 - val_accuracy: 0.8872\n",
            "Epoch 181/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4341 - accuracy: 0.8834 - val_loss: 0.4457 - val_accuracy: 0.8864\n",
            "Epoch 182/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4457 - accuracy: 0.8796 - val_loss: 0.4516 - val_accuracy: 0.8858\n",
            "Epoch 183/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4340 - accuracy: 0.8844 - val_loss: 0.4368 - val_accuracy: 0.8896\n",
            "Epoch 184/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4423 - accuracy: 0.8791 - val_loss: 0.4455 - val_accuracy: 0.8867\n",
            "Epoch 185/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4371 - accuracy: 0.8816 - val_loss: 0.4326 - val_accuracy: 0.8906\n",
            "Epoch 186/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4358 - accuracy: 0.8821 - val_loss: 0.4382 - val_accuracy: 0.8902\n",
            "Epoch 187/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4391 - accuracy: 0.8806 - val_loss: 0.4492 - val_accuracy: 0.8851\n",
            "Epoch 188/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4445 - accuracy: 0.8799 - val_loss: 0.4678 - val_accuracy: 0.8807\n",
            "Epoch 189/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4409 - accuracy: 0.8815 - val_loss: 0.4760 - val_accuracy: 0.8809\n",
            "Epoch 190/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4436 - accuracy: 0.8800 - val_loss: 0.4181 - val_accuracy: 0.8948\n",
            "Epoch 191/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4352 - accuracy: 0.8831 - val_loss: 0.4493 - val_accuracy: 0.8866\n",
            "Epoch 192/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4345 - accuracy: 0.8820 - val_loss: 0.4385 - val_accuracy: 0.8874\n",
            "Epoch 193/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4444 - accuracy: 0.8796 - val_loss: 0.4482 - val_accuracy: 0.8866\n",
            "Epoch 194/300\n",
            "781/781 [==============================] - 28s 36ms/step - loss: 0.4325 - accuracy: 0.8843 - val_loss: 0.4824 - val_accuracy: 0.8777\n",
            "Epoch 195/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4338 - accuracy: 0.8828 - val_loss: 0.4379 - val_accuracy: 0.8876\n",
            "Epoch 196/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4318 - accuracy: 0.8855 - val_loss: 0.4614 - val_accuracy: 0.8844\n",
            "Epoch 197/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4400 - accuracy: 0.8806 - val_loss: 0.4344 - val_accuracy: 0.8914\n",
            "Epoch 198/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4396 - accuracy: 0.8791 - val_loss: 0.4616 - val_accuracy: 0.8822\n",
            "Epoch 199/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4304 - accuracy: 0.8823 - val_loss: 0.4173 - val_accuracy: 0.8940\n",
            "Epoch 200/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4366 - accuracy: 0.8815 - val_loss: 0.4482 - val_accuracy: 0.8872\n",
            "Epoch 201/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4285 - accuracy: 0.8840 - val_loss: 0.4248 - val_accuracy: 0.8934\n",
            "Epoch 202/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4243 - accuracy: 0.8872 - val_loss: 0.4698 - val_accuracy: 0.8794\n",
            "Epoch 203/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4429 - accuracy: 0.8802 - val_loss: 0.4122 - val_accuracy: 0.8932\n",
            "Epoch 204/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4290 - accuracy: 0.8831 - val_loss: 0.4396 - val_accuracy: 0.8854\n",
            "Epoch 205/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4320 - accuracy: 0.8853 - val_loss: 0.4609 - val_accuracy: 0.8815\n",
            "Epoch 206/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4347 - accuracy: 0.8838 - val_loss: 0.4394 - val_accuracy: 0.8848\n",
            "Epoch 207/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4355 - accuracy: 0.8820 - val_loss: 0.4328 - val_accuracy: 0.8859\n",
            "Epoch 208/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4243 - accuracy: 0.8846 - val_loss: 0.4404 - val_accuracy: 0.8870\n",
            "Epoch 209/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4326 - accuracy: 0.8818 - val_loss: 0.4369 - val_accuracy: 0.8893\n",
            "Epoch 210/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4295 - accuracy: 0.8837 - val_loss: 0.3977 - val_accuracy: 0.8980\n",
            "Epoch 211/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4260 - accuracy: 0.8846 - val_loss: 0.4507 - val_accuracy: 0.8855\n",
            "Epoch 212/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4364 - accuracy: 0.8812 - val_loss: 0.4554 - val_accuracy: 0.8834\n",
            "Epoch 213/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4318 - accuracy: 0.8813 - val_loss: 0.4730 - val_accuracy: 0.8785\n",
            "Epoch 214/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4356 - accuracy: 0.8797 - val_loss: 0.4398 - val_accuracy: 0.8921\n",
            "Epoch 215/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4419 - accuracy: 0.8810 - val_loss: 0.4357 - val_accuracy: 0.8896\n",
            "Epoch 216/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4287 - accuracy: 0.8860 - val_loss: 0.4519 - val_accuracy: 0.8898\n",
            "Epoch 217/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4304 - accuracy: 0.8830 - val_loss: 0.4444 - val_accuracy: 0.8883\n",
            "Epoch 218/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4306 - accuracy: 0.8835 - val_loss: 0.4257 - val_accuracy: 0.8900\n",
            "Epoch 219/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4221 - accuracy: 0.8871 - val_loss: 0.4500 - val_accuracy: 0.8855\n",
            "Epoch 220/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4300 - accuracy: 0.8835 - val_loss: 0.4437 - val_accuracy: 0.8866\n",
            "Epoch 221/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4260 - accuracy: 0.8852 - val_loss: 0.4299 - val_accuracy: 0.8926\n",
            "Epoch 222/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4254 - accuracy: 0.8854 - val_loss: 0.4516 - val_accuracy: 0.8835\n",
            "Epoch 223/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4232 - accuracy: 0.8851 - val_loss: 0.4376 - val_accuracy: 0.8874\n",
            "Epoch 224/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4266 - accuracy: 0.8844 - val_loss: 0.4432 - val_accuracy: 0.8860\n",
            "Epoch 225/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4264 - accuracy: 0.8823 - val_loss: 0.4130 - val_accuracy: 0.8936\n",
            "Epoch 226/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4259 - accuracy: 0.8853 - val_loss: 0.4276 - val_accuracy: 0.8910\n",
            "Epoch 227/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4279 - accuracy: 0.8840 - val_loss: 0.4444 - val_accuracy: 0.8862\n",
            "Epoch 228/300\n",
            "781/781 [==============================] - 30s 39ms/step - loss: 0.4267 - accuracy: 0.8848 - val_loss: 0.4294 - val_accuracy: 0.8896\n",
            "Epoch 229/300\n",
            "781/781 [==============================] - 31s 39ms/step - loss: 0.4244 - accuracy: 0.8844 - val_loss: 0.4457 - val_accuracy: 0.8852\n",
            "Epoch 230/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4264 - accuracy: 0.8841 - val_loss: 0.4314 - val_accuracy: 0.8889\n",
            "Epoch 231/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4275 - accuracy: 0.8845 - val_loss: 0.4398 - val_accuracy: 0.8907\n",
            "Epoch 232/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4296 - accuracy: 0.8824 - val_loss: 0.4534 - val_accuracy: 0.8853\n",
            "Epoch 233/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4235 - accuracy: 0.8859 - val_loss: 0.4574 - val_accuracy: 0.8857\n",
            "Epoch 234/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4237 - accuracy: 0.8855 - val_loss: 0.4404 - val_accuracy: 0.8899\n",
            "Epoch 235/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4239 - accuracy: 0.8866 - val_loss: 0.4507 - val_accuracy: 0.8861\n",
            "Epoch 236/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4291 - accuracy: 0.8839 - val_loss: 0.4435 - val_accuracy: 0.8866\n",
            "Epoch 237/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4161 - accuracy: 0.8860 - val_loss: 0.4344 - val_accuracy: 0.8895\n",
            "Epoch 238/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4293 - accuracy: 0.8840 - val_loss: 0.3960 - val_accuracy: 0.8984\n",
            "Epoch 239/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4170 - accuracy: 0.8876 - val_loss: 0.4313 - val_accuracy: 0.8932\n",
            "Epoch 240/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4302 - accuracy: 0.8832 - val_loss: 0.4417 - val_accuracy: 0.8861\n",
            "Epoch 241/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4237 - accuracy: 0.8853 - val_loss: 0.4073 - val_accuracy: 0.8953\n",
            "Epoch 242/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4303 - accuracy: 0.8830 - val_loss: 0.4576 - val_accuracy: 0.8830\n",
            "Epoch 243/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4227 - accuracy: 0.8861 - val_loss: 0.4141 - val_accuracy: 0.8942\n",
            "Epoch 244/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4241 - accuracy: 0.8834 - val_loss: 0.4593 - val_accuracy: 0.8848\n",
            "Epoch 245/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4209 - accuracy: 0.8865 - val_loss: 0.4297 - val_accuracy: 0.8895\n",
            "Epoch 246/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4136 - accuracy: 0.8886 - val_loss: 0.4291 - val_accuracy: 0.8933\n",
            "Epoch 247/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4202 - accuracy: 0.8860 - val_loss: 0.4322 - val_accuracy: 0.8898\n",
            "Epoch 248/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4164 - accuracy: 0.8868 - val_loss: 0.4489 - val_accuracy: 0.8860\n",
            "Epoch 249/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4259 - accuracy: 0.8823 - val_loss: 0.4280 - val_accuracy: 0.8936\n",
            "Epoch 250/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4267 - accuracy: 0.8858 - val_loss: 0.4347 - val_accuracy: 0.8911\n",
            "Epoch 251/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4276 - accuracy: 0.8820 - val_loss: 0.4357 - val_accuracy: 0.8879\n",
            "Epoch 252/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4203 - accuracy: 0.8873 - val_loss: 0.4121 - val_accuracy: 0.8970\n",
            "Epoch 253/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4222 - accuracy: 0.8870 - val_loss: 0.4171 - val_accuracy: 0.8935\n",
            "Epoch 254/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4231 - accuracy: 0.8852 - val_loss: 0.4512 - val_accuracy: 0.8858\n",
            "Epoch 255/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4188 - accuracy: 0.8868 - val_loss: 0.4372 - val_accuracy: 0.8908\n",
            "Epoch 256/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4141 - accuracy: 0.8881 - val_loss: 0.4212 - val_accuracy: 0.8924\n",
            "Epoch 257/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4321 - accuracy: 0.8844 - val_loss: 0.4382 - val_accuracy: 0.8862\n",
            "Epoch 258/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4197 - accuracy: 0.8886 - val_loss: 0.4762 - val_accuracy: 0.8822\n",
            "Epoch 259/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4122 - accuracy: 0.8880 - val_loss: 0.4558 - val_accuracy: 0.8834\n",
            "Epoch 260/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4244 - accuracy: 0.8849 - val_loss: 0.4301 - val_accuracy: 0.8925\n",
            "Epoch 261/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4196 - accuracy: 0.8871 - val_loss: 0.4224 - val_accuracy: 0.8937\n",
            "Epoch 262/300\n",
            "781/781 [==============================] - 30s 39ms/step - loss: 0.4223 - accuracy: 0.8871 - val_loss: 0.4242 - val_accuracy: 0.8906\n",
            "Epoch 263/300\n",
            "781/781 [==============================] - 31s 39ms/step - loss: 0.4226 - accuracy: 0.8852 - val_loss: 0.4066 - val_accuracy: 0.8966\n",
            "Epoch 264/300\n",
            "781/781 [==============================] - 30s 39ms/step - loss: 0.4186 - accuracy: 0.8885 - val_loss: 0.4312 - val_accuracy: 0.8912\n",
            "Epoch 265/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4255 - accuracy: 0.8847 - val_loss: 0.4490 - val_accuracy: 0.8828\n",
            "Epoch 266/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4158 - accuracy: 0.8871 - val_loss: 0.4204 - val_accuracy: 0.8937\n",
            "Epoch 267/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4143 - accuracy: 0.8862 - val_loss: 0.4249 - val_accuracy: 0.8922\n",
            "Epoch 268/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4166 - accuracy: 0.8886 - val_loss: 0.4447 - val_accuracy: 0.8861\n",
            "Epoch 269/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4088 - accuracy: 0.8881 - val_loss: 0.4115 - val_accuracy: 0.8985\n",
            "Epoch 270/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4220 - accuracy: 0.8862 - val_loss: 0.4013 - val_accuracy: 0.8976\n",
            "Epoch 271/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4200 - accuracy: 0.8857 - val_loss: 0.4219 - val_accuracy: 0.8959\n",
            "Epoch 272/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4192 - accuracy: 0.8868 - val_loss: 0.4472 - val_accuracy: 0.8892\n",
            "Epoch 273/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4110 - accuracy: 0.8881 - val_loss: 0.4040 - val_accuracy: 0.8986\n",
            "Epoch 274/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4124 - accuracy: 0.8886 - val_loss: 0.4169 - val_accuracy: 0.8958\n",
            "Epoch 275/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4088 - accuracy: 0.8890 - val_loss: 0.4392 - val_accuracy: 0.8867\n",
            "Epoch 276/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4168 - accuracy: 0.8876 - val_loss: 0.4076 - val_accuracy: 0.8960\n",
            "Epoch 277/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4270 - accuracy: 0.8822 - val_loss: 0.4433 - val_accuracy: 0.8912\n",
            "Epoch 278/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4187 - accuracy: 0.8887 - val_loss: 0.4530 - val_accuracy: 0.8863\n",
            "Epoch 279/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4220 - accuracy: 0.8851 - val_loss: 0.4571 - val_accuracy: 0.8867\n",
            "Epoch 280/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4180 - accuracy: 0.8858 - val_loss: 0.4189 - val_accuracy: 0.8940\n",
            "Epoch 281/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4202 - accuracy: 0.8855 - val_loss: 0.4225 - val_accuracy: 0.8917\n",
            "Epoch 282/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4196 - accuracy: 0.8862 - val_loss: 0.4271 - val_accuracy: 0.8919\n",
            "Epoch 283/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4152 - accuracy: 0.8875 - val_loss: 0.4274 - val_accuracy: 0.8947\n",
            "Epoch 284/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4192 - accuracy: 0.8863 - val_loss: 0.4124 - val_accuracy: 0.8960\n",
            "Epoch 285/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4129 - accuracy: 0.8877 - val_loss: 0.4240 - val_accuracy: 0.8961\n",
            "Epoch 286/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4040 - accuracy: 0.8909 - val_loss: 0.3983 - val_accuracy: 0.8998\n",
            "Epoch 287/300\n",
            "781/781 [==============================] - 29s 37ms/step - loss: 0.4185 - accuracy: 0.8862 - val_loss: 0.4527 - val_accuracy: 0.8840\n",
            "Epoch 288/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4145 - accuracy: 0.8868 - val_loss: 0.4362 - val_accuracy: 0.8891\n",
            "Epoch 289/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4094 - accuracy: 0.8901 - val_loss: 0.4199 - val_accuracy: 0.8932\n",
            "Epoch 290/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4197 - accuracy: 0.8862 - val_loss: 0.4170 - val_accuracy: 0.8952\n",
            "Epoch 291/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4081 - accuracy: 0.8913 - val_loss: 0.4129 - val_accuracy: 0.8979\n",
            "Epoch 292/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4134 - accuracy: 0.8879 - val_loss: 0.4232 - val_accuracy: 0.8945\n",
            "Epoch 293/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4132 - accuracy: 0.8888 - val_loss: 0.4210 - val_accuracy: 0.8944\n",
            "Epoch 294/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4033 - accuracy: 0.8922 - val_loss: 0.4249 - val_accuracy: 0.8923\n",
            "Epoch 295/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4089 - accuracy: 0.8896 - val_loss: 0.4275 - val_accuracy: 0.8906\n",
            "Epoch 296/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4132 - accuracy: 0.8899 - val_loss: 0.4211 - val_accuracy: 0.8934\n",
            "Epoch 297/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4134 - accuracy: 0.8874 - val_loss: 0.4043 - val_accuracy: 0.8982\n",
            "Epoch 298/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4112 - accuracy: 0.8895 - val_loss: 0.3954 - val_accuracy: 0.9013\n",
            "Epoch 299/300\n",
            "781/781 [==============================] - 29s 38ms/step - loss: 0.4244 - accuracy: 0.8867 - val_loss: 0.4380 - val_accuracy: 0.8878\n",
            "Epoch 300/300\n",
            "781/781 [==============================] - 30s 38ms/step - loss: 0.4174 - accuracy: 0.8885 - val_loss: 0.3913 - val_accuracy: 0.9006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeautW_PrjV-",
        "outputId": "5570eab4-45de-45a4-da16-7a0b03891e42"
      },
      "source": [
        " #testing\n",
        "scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)\n",
        "print('\\nTest result: %.3f loss: %.3f' % (scores[1]*100,scores[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 0s 6ms/step - loss: 0.3913 - accuracy: 0.9006\n",
            "\n",
            "Test result: 90.060 loss: 0.391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "sgmA5dPgwjnH",
        "outputId": "8f0466b4-7e3f-41bd-a3d3-a002405f33b0"
      },
      "source": [
        "from matplotlib import pyplot\n",
        "\n",
        "model.load_weights('model.h5')\n",
        " \n",
        "labels =  ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        " \n",
        "indices = np.argmax(model.predict(x_test[:26]),1)\n",
        "print([labels[x] for x in indices])\n",
        "imshow(x_test[2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['cat', 'ship', 'ship', 'ship', 'frog', 'frog', 'automobile', 'frog', 'cat', 'automobile', 'airplane', 'truck', 'dog', 'horse', 'truck', 'ship', 'dog', 'horse', 'ship', 'frog', 'horse', 'airplane', 'deer', 'truck', 'dog', 'bird']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8817431250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZZ4of8yRddg"
      },
      "source": [
        "indices = np.argmax(model.predict(x_test[0:1]),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAsPIwNptH19",
        "outputId": "176da450-0194-4b35-9310-7135a9259c4d"
      },
      "source": [
        "labels =  ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "print([labels[x] for x in indices])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['airplane']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd44c106250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIQohKDavAq8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}